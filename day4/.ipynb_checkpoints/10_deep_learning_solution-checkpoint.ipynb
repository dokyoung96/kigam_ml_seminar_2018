{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 â€“ Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.899\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9198\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add summary for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.899\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9198\n",
      "2 Train accuracy: 0.98 Test accuracy: 0.928\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9358\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9422\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9466\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9504\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9538\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.957\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9594\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   #-----new------#\n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      #-----new------#\n",
    "\n",
    "root_logdir = \"./logs\"                                          #-----new------#\n",
    "logdir = \"{}/{}_{}\".format(root_logdir, \"relu\", now)   #-----new------#\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                    #-----new------#\n",
    "loss_summary = tf.summary.scalar('loss', loss)                            #-----new------#\n",
    "merged = tf.summary.merge_all()                                            #-----new------#\n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())        #-----new------#\n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        #-----new------#\n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch})    #-----new------# \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     #-----new------#\n",
    "        train_writer.add_summary(summary_train, epoch)       #-----new------#\n",
    "        test_writer.add_summary(summary_test, epoch)         #-----new------#\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.9249\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9417\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9504\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.955\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9583\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9614\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9621\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9636\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9641\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9653\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   #-----new------#\n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      #-----new------#\n",
    "\n",
    "root_logdir = \"./logs\"                                          #-----new------#\n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"relu\", \"std\", now)   #-----new------#\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                    #-----new------#\n",
    "loss_summary = tf.summary.scalar('loss', loss)                            #-----new------#\n",
    "merged = tf.summary.merge_all()                                            #-----new------#\n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())        #-----new------#\n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        #-----new------#\n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch})    #-----new------# \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test_scaled, y: y_test})     #-----new------#\n",
    "        train_writer.add_summary(summary_train, epoch)       #-----new------#\n",
    "        test_writer.add_summary(summary_test, epoch)         #-----new------#\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 1.0 Test accuracy: 0.9234\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9432\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9496\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9537\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9559\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9586\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9605\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9628\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9638\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}\".format(root_logdir, \"relu\", \"He\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                     #-----new------#\n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", kernel_initializer=he_init, #-----new------#\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", kernel_initializer=he_init, #-----new------#\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, kernel_initializer=he_init, name=\"outputs\")  #-----new------#\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch})    \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEJCAYAAAC0U81tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuEVEYkbFLSISMypuEZGY+T9AAQTm4aRe1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f80dda0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9223\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9372\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9467\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9518\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9558\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9592\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9613\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9627\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.964\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"lrelu\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=leaky_relu)                                         #-----new------#\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=leaky_relu)                                         #-----new------#\n",
    "    logits = tf.layers.dense(hidden2, n_outputs,  name=\"outputs\")  #-----new------#\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAELCAYAAADN4q16AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMoO/LwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f809e6208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.9235\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9381\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9462\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9514\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9541\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9568\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9602\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9619\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9626\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9642\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"elu\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.elu)                                         #-----new------#\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.elu)                                         #-----new------#\n",
    "    logits = tf.layers.dense(hidden2, n_outputs,  name=\"outputs\")  \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEJCAYAAACAKgxxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f800977f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.9193\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9339\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9426\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.95\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.954\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9563\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9586\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.96\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9614\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.963\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"selu\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=selu)                                         #-----new------#\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=selu)                                         #-----new------#\n",
    "    logits = tf.layers.dense(hidden2, n_outputs,  name=\"outputs\")  \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9092\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9302\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9433\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9512\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9573\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9603\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9626\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9647\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9656\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9668\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}\".format(root_logdir, \"elu\", \"std\", \"BN\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')        #-----new------#\n",
    "batch_norm_momentum = 0.9                                   #-----new------#\n",
    "from functools import partial                             #-----new------#\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "    #-----------------------------new-----------------------------------#\n",
    "    my_batch_norm_layer = partial(                                \n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    #-------------------------------------------------------------------#\n",
    "    \n",
    "#     hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")                                         \n",
    "#     bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "#     elu1 = tf.nn.elu(bn1)\n",
    "    \n",
    "#     hidden2 = tf.layers.dense(elu1, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")                                         \n",
    "#     bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "#     elu2 = tf.nn.elu(bn2)\n",
    "    \n",
    "#     logits_before_bn = tf.layers.dense(elu2, n_outputs, kernel_initializer=he_init, name=\"outputs\")  \n",
    "#     logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)            #-----new------#\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) #-----new------#\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.94 Test accuracy: 0.9512\n",
      "1 Train accuracy: 0.96 Test accuracy: 0.9624\n",
      "2 Train accuracy: 0.98 Test accuracy: 0.9694\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.972\n",
      "4 Train accuracy: 0.98 Test accuracy: 0.9679\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9693\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9706\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9769\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}_{}\".format(root_logdir, \"elu\", \"std\", \"BN\", 'Adam', now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')        \n",
    "batch_norm_momentum = 0.9                                   \n",
    "from functools import partial                            \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")                                         \n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    elu1 = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(elu1, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")                                         \n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    elu2 = tf.nn.elu(bn2)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(elu2, n_outputs, kernel_initializer=he_init, name=\"outputs\")  \n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)           #-----new------#\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)           \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) \n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9151\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9368\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9466\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9529\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9583\n",
      "5 Train accuracy: 0.98 Test accuracy: 0.9618\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9656\n",
      "7 Train accuracy: 0.98 Test accuracy: 0.965\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9683\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9701\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}_{}\".format(root_logdir, \"elu\", \"std\", \"BN\", 'Adam', now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')        \n",
    "batch_norm_momentum = 0.9                                   \n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob                            #-----new------#\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)      #-----new------#\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")                                         \n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    elu1 = tf.nn.elu(bn1)\n",
    "    hidden1_drop = tf.layers.dropout(elu1, dropout_rate, training=training)            #-----new------#\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")            #-----new------#                              \n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    elu2 = tf.nn.elu(bn2)\n",
    "    hidden2_drop = tf.layers.dropout(elu2, dropout_rate, training=training)            #-----new------#\n",
    "\n",
    "    logits_before_bn = tf.layers.dense(hidden2_drop, n_outputs, kernel_initializer=he_init, name=\"outputs\")   #-----new------#\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)           \n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)           \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) \n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
