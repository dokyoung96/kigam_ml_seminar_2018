{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 â€“ Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.899\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9198\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add summary for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.899\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9198\n",
      "2 Train accuracy: 0.98 Test accuracy: 0.928\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9358\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9422\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9466\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9504\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9538\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.957\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9594\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   #-----new------#\n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      #-----new------#\n",
    "\n",
    "root_logdir = \"./logs\"                                          #-----new------#\n",
    "logdir = \"{}/{}_{}\".format(root_logdir, \"relu\", now)   #-----new------#\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                    #-----new------#\n",
    "loss_summary = tf.summary.scalar('loss', loss)                            #-----new------#\n",
    "merged = tf.summary.merge_all()                                            #-----new------#\n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())        #-----new------#\n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        #-----new------#\n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch})    #-----new------# \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     #-----new------#\n",
    "        train_writer.add_summary(summary_train, epoch)       #-----new------#\n",
    "        test_writer.add_summary(summary_test, epoch)         #-----new------#\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.9249\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9417\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9504\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.955\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9583\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9614\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9621\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9636\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9641\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9653\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   #-----new------#\n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      #-----new------#\n",
    "\n",
    "root_logdir = \"./logs\"                                          #-----new------#\n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"relu\", \"std\", now)   #-----new------#\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                    #-----new------#\n",
    "loss_summary = tf.summary.scalar('loss', loss)                            #-----new------#\n",
    "merged = tf.summary.merge_all()                                            #-----new------#\n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())        #-----new------#\n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        #-----new------#\n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch})    #-----new------# \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test_scaled, y: y_test})     #-----new------#\n",
    "        train_writer.add_summary(summary_train, epoch)       #-----new------#\n",
    "        test_writer.add_summary(summary_test, epoch)         #-----new------#\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 1.0 Test accuracy: 0.9234\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9432\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9496\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9537\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9559\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9586\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9605\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9628\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9638\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}\".format(root_logdir, \"relu\", \"He\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                     #-----new------#\n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", kernel_initializer=he_init, #-----new------#\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", kernel_initializer=he_init, #-----new------#\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, kernel_initializer=he_init, name=\"outputs\")  #-----new------#\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch})    \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEJCAYAAAC0U81tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM/8fkuoJwBJA0yEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ca44e22e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9223\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9372\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9467\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9518\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9558\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9592\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9613\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9627\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.964\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"lrelu\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=leaky_relu)                                         #-----new------#\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=leaky_relu)                                         #-----new------#\n",
    "    logits = tf.layers.dense(hidden2, n_outputs,  name=\"outputs\")  #-----new------#\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAELCAYAAADN4q16AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ca4419b7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.9235\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9381\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9462\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9514\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9541\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9568\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9602\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9619\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9626\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9642\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"elu\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.elu)                                         #-----new------#\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.elu)                                         #-----new------#\n",
    "    logits = tf.layers.dense(hidden2, n_outputs,  name=\"outputs\")  \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEJCAYAAACAKgxxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDKPELiKSYZTYRUQyzP8D8rYYi1F7j+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ca462942e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Test accuracy: 0.9193\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9339\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9426\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.95\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.954\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9563\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9586\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.96\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9614\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.963\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}\".format(root_logdir, \"selu\", \"std\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=selu)                                         #-----new------#\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=selu)                                         #-----new------#\n",
    "    logits = tf.layers.dense(hidden2, n_outputs,  name=\"outputs\")  \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run(training_op, feed_dict={X: scaler.transform(X_batch), y: y_batch})\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_batch), y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: scaler.transform(X_test), y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9092\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9302\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9433\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9512\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9573\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9603\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9626\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9647\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9656\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9668\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}\".format(root_logdir, \"elu\", \"std\", \"BN\", now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')        #-----new------#\n",
    "batch_norm_momentum = 0.9                                   #-----new------#\n",
    "from functools import partial                             #-----new------#\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "    #-----------------------------new-----------------------------------#\n",
    "    my_batch_norm_layer = partial(                                \n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    #-------------------------------------------------------------------#\n",
    "    \n",
    "#     hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")                                         \n",
    "#     bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "#     elu1 = tf.nn.elu(bn1)\n",
    "    \n",
    "#     hidden2 = tf.layers.dense(elu1, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")                                         \n",
    "#     bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "#     elu2 = tf.nn.elu(bn2)\n",
    "    \n",
    "#     logits_before_bn = tf.layers.dense(elu2, n_outputs, kernel_initializer=he_init, name=\"outputs\")  \n",
    "#     logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)            #-----new------#\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) #-----new------#\n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.94 Test accuracy: 0.9512\n",
      "1 Train accuracy: 0.96 Test accuracy: 0.9624\n",
      "2 Train accuracy: 0.98 Test accuracy: 0.9694\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.972\n",
      "4 Train accuracy: 0.98 Test accuracy: 0.9679\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9693\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9706\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9769\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}_{}\".format(root_logdir, \"elu\", \"std\", \"BN\", 'Adam', now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')        \n",
    "batch_norm_momentum = 0.9                                   \n",
    "from functools import partial                            \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")                                         \n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    elu1 = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(elu1, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")                                         \n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    elu2 = tf.nn.elu(bn2)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(elu2, n_outputs, kernel_initializer=he_init, name=\"outputs\")  \n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)           #-----new------#\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)           \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) \n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9151\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9368\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9466\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9529\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.9583\n",
      "5 Train accuracy: 0.98 Test accuracy: 0.9618\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9656\n",
      "7 Train accuracy: 0.98 Test accuracy: 0.965\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9683\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9701\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "from datetime import datetime                                   \n",
    "now = datetime.utcnow().strftime(\"%H%M%S\")                      \n",
    "\n",
    "root_logdir = \"./logs\"                                         \n",
    "logdir = \"{}/{}_{}_{}_{}_{}\".format(root_logdir, \"elu\", \"std\", \"BN\", 'Adam', now)  \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')        \n",
    "batch_norm_momentum = 0.9                                   \n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob                            #-----new------#\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)      #-----new------#\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()                    \n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")                                         \n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    elu1 = tf.nn.elu(bn1)\n",
    "    hidden1_drop = tf.layers.dropout(elu1, dropout_rate, training=training)            #-----new------#\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")            #-----new------#                              \n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    elu2 = tf.nn.elu(bn2)\n",
    "    hidden2_drop = tf.layers.dropout(elu2, dropout_rate, training=training)            #-----new------#\n",
    "\n",
    "    logits_before_bn = tf.layers.dense(hidden2_drop, n_outputs, kernel_initializer=he_init, name=\"outputs\")   #-----new------#\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)           \n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)                   \n",
    "loss_summary = tf.summary.scalar('loss', loss)                          \n",
    "merged = tf.summary.merge_all()                                         \n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train', tf.get_default_graph())  \n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')                        \n",
    " \n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)           \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) \n",
    "            \n",
    "        acc_train, summary_train = sess.run([accuracy, merged], feed_dict={X: X_batch, y: y_batch}) \n",
    "        acc_test, summary_test = sess.run([accuracy, merged], feed_dict={X: X_test, y: y_test})     \n",
    "        train_writer.add_summary(summary_train, epoch)       \n",
    "        test_writer.add_summary(summary_test, epoch)         \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
